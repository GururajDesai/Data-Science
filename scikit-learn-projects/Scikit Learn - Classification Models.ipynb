{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import itertools\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the file\n",
    "data = pd.read_csv(\"./sample file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shuffle Data\n",
    "from sklearn.utils import shuffle\n",
    "data = shuffle(data)\n",
    "x = data.iloc[:,0:16]\n",
    "y = data.iloc[:,-1]\n",
    "\n",
    "#Split data and create Train and Test Dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.30, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validaion(model, x, y):\n",
    "    from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    import pickle\n",
    "    scores = cross_val_score(model, x, y, cv=5)\n",
    "    print(\"K FOLD ACCURACY\")\n",
    "    print(scores.mean())\n",
    "    print(\"**********\")\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "    print(\"Save the Model\")\n",
    "    #return scores.mean()\n",
    "    y_pred = cross_val_predict(model, x, y, cv=5)\n",
    "    conf_mat = confusion_matrix(y, y_pred)\n",
    "    print(\"K Fold Validation Confusion Matrix\")\n",
    "    plot_confusion_matrix(conf_mat,[0,1])\n",
    "    model.fit(x,y)\n",
    "    pickle.dump(model, open(\"decision_tree_3_vector_model.pkl\", 'wb'))\n",
    "    plot_classification_data(model, x, y)\n",
    "    return scores.mean()\n",
    "\n",
    "\n",
    "def plot_classification_data(model, x, y):\n",
    "    from sklearn.decomposition import PCA\n",
    "    predicted_values = model.predict(x)\n",
    "    pca = PCA(n_components=2)\n",
    "    proj = pca.fit_transform(x)\n",
    "    plt.scatter(proj[:, 0], proj[:, 1], c=y, cmap=\"Paired\")\n",
    "    plt.colorbar()\n",
    "    plt.scatter(proj[:, 0], proj[:, 1], c=predicted_values, cmap=\"Paired\")\n",
    "    plt.colorbar()\n",
    "    \n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.figure()\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def get_confusion_matrix(y_true, y_pred):\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    cnf = confusion_matrix(y_true, y_pred)\n",
    "    plot_confusion_matrix(cnf, [0,1])\n",
    "    \n",
    "def print_decision_tree(model, filename):\n",
    "    '''\n",
    "    to store and open decision tree\n",
    "    brew install graphviz\n",
    "    tree.export_graphviz(clf,out_file='./decision_tree.dot')\n",
    "    dot -Tpng tree.dot -o tree.png\n",
    "    run this to install graphviz\n",
    "    sudo chown -R $(whoami) /usr/local/share/info /usr/local/share/man/man3 /usr/local/share/man/man5\n",
    "    prints in .dot format\n",
    "    export to png use this command - dot -Tpng filename.dot -o filename.dot\n",
    "    '''\n",
    "    from sklearn import tree\n",
    "    tree.export_graphviz(model,out_file=filename)\n",
    "    \n",
    "#Logistic Regression\n",
    "def logisic_regression(X_train,y_train, X_test, y_test,x ,y):\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    logreg = LogisticRegression(multi_class=\"multinomial\",solver=\"saga\")\n",
    "    logreg.fit(X_train, y_train)\n",
    "    print('Accuracy of Logistic regression classifier on training set: {:.2f}'\n",
    "         .format(logreg.score(X_train, y_train)))\n",
    "    print('Accuracy of Logistic regression classifier on test set: {:.2f}'\n",
    "         .format(logreg.score(X_test, y_test)))\n",
    "    logreg = LogisticRegression(multi_class=\"multinomial\",solver=\"saga\")\n",
    "    cross_validaion(logreg, x, y)\n",
    "    \n",
    "#Decision Tree\n",
    "def decision_tree(X_train,y_train, X_test, y_test, x, y):\n",
    "    '''\n",
    "    for decision tree the cross validaion is done to get the error estimate but the tree will be trainined\n",
    "    on the entire dataset like below\n",
    "    '''\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    clf = DecisionTreeClassifier(max_depth=8).fit(X_train, y_train)\n",
    "    print('Accuracy of Decision Tree classifier on training set: {:.2f}'\n",
    "         .format(clf.score(X_train, y_train)))\n",
    "    print('Accuracy of Decision Tree classifier on test set: {:.2f}'\n",
    "         .format(clf.score(X_test, y_test)))\n",
    "    clf = DecisionTreeClassifier(max_depth=8).fit(x, y)\n",
    "    print(\"important features++++++++++++++\")\n",
    "    print(clf.feature_importances_)\n",
    "    print_decision_tree(clf,\"./decision_tree.dot\")\n",
    "    print(\"Train and Test Validation Confusion Matrix\")\n",
    "    get_confusion_matrix(y,clf.predict(x))\n",
    "    clf = DecisionTreeClassifier(max_depth=8)\n",
    "    return cross_validaion(clf, x, y)\n",
    "    \n",
    "#K nearest neighbours\n",
    "def nearest_neighbours(X_train,y_train, X_test, y_test, x, y):\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    knn = KNeighborsClassifier()\n",
    "    knn.fit(X_train, y_train)\n",
    "    print('Accuracy of K-NN classifier on training set: {:.2f}'\n",
    "         .format(knn.score(X_train, y_train)))\n",
    "    print('Accuracy of K-NN classifier on test set: {:.2f}'\n",
    "         .format(knn.score(X_test, y_test)))\n",
    "    knn = KNeighborsClassifier()\n",
    "    cross_validaion(knn, x, y)\n",
    "    \n",
    "def discriminant_analysis(X_train,y_train, X_test, y_test, x, y):\n",
    "    from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "    lda = LinearDiscriminantAnalysis()\n",
    "    lda.fit(X_train, y_train)\n",
    "    print('Accuracy of LDA classifier on training set: {:.2f}'\n",
    "         .format(lda.score(X_train, y_train)))\n",
    "    print('Accuracy of LDA classifier on test set: {:.2f}'\n",
    "         .format(lda.score(X_test, y_test)))\n",
    "    lda = LinearDiscriminantAnalysis()\n",
    "    cross_validaion(lda, x, y)\n",
    "\n",
    "def guasian_naive_bayes(X_train,y_train, X_test, y_test, x, y):\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "    gnb = GaussianNB()\n",
    "    gnb.fit(X_train, y_train)\n",
    "    print('Accuracy of GNB classifier on training set: {:.2f}'\n",
    "         .format(gnb.score(X_train, y_train)))\n",
    "    print('Accuracy of GNB classifier on test set: {:.2f}'\n",
    "         .format(gnb.score(X_test, y_test)))\n",
    "    gnb = GaussianNB()\n",
    "    cross_validaion(gnb, x, y)\n",
    "    \n",
    "def neural_net(X_train, y_train, X_test, y_test, x, y):\n",
    "    clf = MLPClassifier(solver='sgd', alpha=0.001,hidden_layer_sizes=(32,16,2), random_state=1)   \n",
    "    clf.fit(X_train, y_train)\n",
    "    predicted_values = clf.predict(X_test)\n",
    "    actual_values = y_test\n",
    "    print(\"Accuracy of Neural Network classifier on train set: {}\".format(accuracy_score(y_train, clf.predict(X_train))))\n",
    "    print(\"Accuracy of Neural Network classifier on test set: {}\".format(accuracy_score(actual_values, predicted_values)))\n",
    "    clf = MLPClassifier(solver='sgd', alpha=0.001,hidden_layer_sizes=(32,16,2), random_state=1)\n",
    "    cross_validaion(clf, x, y)\n",
    "    \n",
    "def svm(X_train, y_train, X_test, y_test, x, y):\n",
    "    from sklearn.svm import SVC\n",
    "    svm = SVC()\n",
    "    svm.fit(X_train, y_train)\n",
    "    print('Accuracy of SVM classifier on training set: {:.2f}'\n",
    "         .format(svm.score(X_train, y_train)))\n",
    "    print('Accuracy of SVM classifier on test set: {:.2f}'\n",
    "         .format(svm.score(X_test, y_test)))\n",
    "    svm = SVC().fit(x, y)\n",
    "    print(\"Train And Test Confusion Matrix\")\n",
    "    get_confusion_matrix(y,svm.predict(x))\n",
    "    svm = SVC()\n",
    "    return cross_validaion(svm, x, y)\n",
    "\n",
    "def convert_labels_to_one_not_encode(labels):\n",
    "    trainLabels = []\n",
    "    for i in range(0, len(labels)):\n",
    "        lab = labels[i]\n",
    "        if lab == 0:\n",
    "            trainLabels.append(np.array([1,0]))\n",
    "        else:\n",
    "            trainLabels.append(np.array([0,1]))\n",
    "    trainLabels = np.array(trainLabels)\n",
    "    return trainLabels\n",
    "\n",
    "def random_forest_classifier(X_train,y_train, X_test, y_test,x , y):\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    #n_estimators mean n different trees\n",
    "    random_forest = RandomForestClassifier(n_estimators=20, max_depth=5).fit(X_train, y_train)\n",
    "    print('Accuracy of Decision Tree classifier on training set: {:.2f}'\n",
    "         .format(random_forest.score(X_train, y_train)))\n",
    "    print('Accuracy of Decision Tree classifier on test set: {:.2f}'\n",
    "         .format(random_forest.score(X_test, y_test)))\n",
    "    random_forest = RandomForestClassifier(n_estimators=20, max_depth=5)\n",
    "    cross_validaion(random_forest, x, y)\n",
    "    \n",
    "def bagging_classifier(X_train,y_train, X_test, y_test,x,y):\n",
    "    from sklearn.ensemble import BaggingClassifier\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    bag_classifier = BaggingClassifier(DecisionTreeClassifier(max_depth=5), max_samples=0.5, max_features=1.0, n_estimators=20)\n",
    "    bag_classifier.fit(X_train,y_train)\n",
    "    print('Accuracy of Decision Tree classifier on training set: {:.2f}'\n",
    "         .format(bag_classifier.score(X_train, y_train)))\n",
    "    print('Accuracy of Decision Tree classifier on test set: {:.2f}'\n",
    "         .format(bag_classifier.score(X_test, y_test)))\n",
    "    bag_classifier = BaggingClassifier(DecisionTreeClassifier(max_depth=5), max_samples=0.5, max_features=1.0, n_estimators=20)\n",
    "    cross_validaion(bag_classifier, x, y)\n",
    "\n",
    "def ada_boost_classifier(X_train,y_train, X_test, y_test,x,y):\n",
    "    #for best algo add max_depth=3 for decisionTree\n",
    "    from sklearn.ensemble import AdaBoostClassifier\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    #use max_depth to avoid overfit\n",
    "    ada_boost = AdaBoostClassifier(DecisionTreeClassifier(max_depth=3), n_estimators=20, learning_rate=1)\n",
    "    ada_boost.fit(X_train,y_train)\n",
    "    print('Accuracy of Decision Tree classifier on training set: {:.2f}'\n",
    "         .format(ada_boost.score(X_train, y_train)))\n",
    "    print('Accuracy of Decision Tree classifier on test set: {:.2f}'\n",
    "         .format(ada_boost.score(X_test, y_test)))\n",
    "    ada_boost = AdaBoostClassifier(DecisionTreeClassifier(max_depth=3), n_estimators=20, learning_rate=1)\n",
    "    ada_boost.fit(x,y)\n",
    "    print(\"Train and Test Validation Confusion Matrix\")\n",
    "    get_confusion_matrix(y,ada_boost.predict(x))\n",
    "    ada_boost = AdaBoostClassifier(DecisionTreeClassifier(max_depth=7), n_estimators=20, learning_rate=1)\n",
    "    return cross_validaion(ada_boost, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example to call any model\n",
    "random_forest_classifier(X_train, y_train, X_test, y_test, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
